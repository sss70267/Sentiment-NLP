{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45760738",
   "metadata": {},
   "source": [
    "# 應用模組"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23ca3778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile   \n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import numpy as np\n",
    "import pyprind\n",
    "import pandas as pd\n",
    "import os,re\n",
    "from nltk.stem.porter import PorterStemmer #波特詞幹還原演算法\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a263a10e",
   "metadata": {},
   "source": [
    "# 資料載入、預處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "284e06b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "             #檔案解壓縮\n",
    "\n",
    "with tarfile.open('aclImdb_v1.tar.gz','r:gz') as tar:\n",
    "    tar.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fc29ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:56\n"
     ]
    }
   ],
   "source": [
    "\n",
    "basepath = 'aclImdb'\n",
    "\n",
    "labels = {'pos':1,'neg':0}\n",
    "bar = pyprind.ProgBar(50000)   #檔案數目\n",
    "df = pd.DataFrame()\n",
    "for s in ('test','train'):\n",
    "    for l in ('pos','neg'):\n",
    "        path = os.path.join(basepath,s,l)\n",
    "        for file  in os.listdir(path):\n",
    "            with open(os.path.join(path,file),'r',encoding= 'utf-8') as infile:  #讀取每個文件內容\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt,labels[l]]],                 #內容、標籤\n",
    "                           ignore_index = True)\n",
    "            bar.update()\n",
    "df.columns = ['review','sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a49d4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentimant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>Towards the end of the movie, I felt it was to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>This is the kind of movie that my enemies cont...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I saw 'Descent' last night at the Stockholm Fi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Some films that you pick up for a pound turn o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>This is one of the dumbest films, I've ever se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentimant\n",
       "0      I went and saw this movie last night after bei...          1\n",
       "1      Actor turned director Bill Paxton follows up h...          1\n",
       "2      As a recreational golfer with some knowledge o...          1\n",
       "3      I saw this film in a sneak preview, and it is ...          1\n",
       "4      Bill Paxton has taken the true story of the 19...          1\n",
       "...                                                  ...        ...\n",
       "49995  Towards the end of the movie, I felt it was to...          0\n",
       "49996  This is the kind of movie that my enemies cont...          0\n",
       "49997  I saw 'Descent' last night at the Stockholm Fi...          0\n",
       "49998  Some films that you pick up for a pound turn o...          0\n",
       "49999  This is one of the dumbest films, I've ever se...          0\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2cfa85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11841</th>\n",
       "      <td>at a Saturday matinee in my home town. I went ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19602</th>\n",
       "      <td>I love this movie. It is the first film Master...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45519</th>\n",
       "      <td>In the voice over which begins the film, Hughi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25747</th>\n",
       "      <td>!!! Spoiler alert!!!&lt;br /&gt;&lt;br /&gt;The point is, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42642</th>\n",
       "      <td>This is an excellent film. No, it's not Mel Gi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31902</th>\n",
       "      <td>This movie is sort of similar to \"Better Off D...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30346</th>\n",
       "      <td>Brown of Harvard is a hard movie to pin down. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12363</th>\n",
       "      <td>No sense going over the story since enough rev...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32490</th>\n",
       "      <td>In this TV special Jon is the one who needs a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26128</th>\n",
       "      <td>Entertainment Tonight has been going down hill...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "11841  at a Saturday matinee in my home town. I went ...          0\n",
       "19602  I love this movie. It is the first film Master...          1\n",
       "45519  In the voice over which begins the film, Hughi...          1\n",
       "25747  !!! Spoiler alert!!!<br /><br />The point is, ...          0\n",
       "42642  This is an excellent film. No, it's not Mel Gi...          1\n",
       "31902  This movie is sort of similar to \"Better Off D...          1\n",
       "30346  Brown of Harvard is a hard movie to pin down. ...          1\n",
       "12363  No sense going over the story since enough rev...          1\n",
       "32490  In this TV special Jon is the one who needs a ...          1\n",
       "26128  Entertainment Tonight has been going down hill...          0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = ['review','sentiment']\n",
    "df.to_csv('movie_data.csv',index = False, encoding = 'utf-8') #存為csv\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50d051a",
   "metadata": {},
   "source": [
    "# 以上只需執行一遍或使用以下CSV載入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e51cf030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11841</th>\n",
       "      <td>at a Saturday matinee in my home town. I went ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19602</th>\n",
       "      <td>I love this movie. It is the first film Master...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45519</th>\n",
       "      <td>In the voice over which begins the film, Hughi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25747</th>\n",
       "      <td>!!! Spoiler alert!!!&lt;br /&gt;&lt;br /&gt;The point is, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42642</th>\n",
       "      <td>This is an excellent film. No, it's not Mel Gi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21243</th>\n",
       "      <td>Although the director tried(the filming was ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45891</th>\n",
       "      <td>It has been about 50 years since a movie has b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42613</th>\n",
       "      <td>\"Bar Hopping\" seems to be trying to be about t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43567</th>\n",
       "      <td>This awful effort just goes to show what happe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>Yes, why? Among the filmmakers that came out i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "11841  at a Saturday matinee in my home town. I went ...          0\n",
       "19602  I love this movie. It is the first film Master...          1\n",
       "45519  In the voice over which begins the film, Hughi...          1\n",
       "25747  !!! Spoiler alert!!!<br /><br />The point is, ...          0\n",
       "42642  This is an excellent film. No, it's not Mel Gi...          1\n",
       "...                                                  ...        ...\n",
       "21243  Although the director tried(the filming was ma...          0\n",
       "45891  It has been about 50 years since a movie has b...          1\n",
       "42613  \"Bar Hopping\" seems to be trying to be about t...          0\n",
       "43567  This awful effort just goes to show what happe...          0\n",
       "2732   Yes, why? Among the filmmakers that came out i...          0\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#隨機排列 讀去資料\n",
    "np.random.seed(0)\n",
    "df = pd.read_csv('movie_data.csv')\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30f16be",
   "metadata": {},
   "source": [
    "# 字詞轉回特徵向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aee7f0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('actors', 0)\n",
      "('age', 1)\n",
      "('and', 2)\n",
      "('cinematography', 3)\n",
      "('delivery', 4)\n",
      "('easy', 5)\n",
      "('effortlessly', 6)\n",
      "('energy', 7)\n",
      "('give', 8)\n",
      "('going', 9)\n",
      "('has', 10)\n",
      "('he', 11)\n",
      "('helped', 12)\n",
      "('him', 13)\n",
      "('his', 14)\n",
      "('in', 15)\n",
      "('kris', 16)\n",
      "('kristofferson', 17)\n",
      "('kudos', 18)\n",
      "('like', 19)\n",
      "('lines', 20)\n",
      "('low', 21)\n",
      "('movies', 22)\n",
      "('must', 23)\n",
      "('of', 24)\n",
      "('really', 25)\n",
      "('scene', 26)\n",
      "('soft', 27)\n",
      "('spoken', 28)\n",
      "('steal', 29)\n",
      "('style', 30)\n",
      "('the', 31)\n",
      "('to', 32)\n",
      "('usual', 33)\n",
      "('will', 34)\n",
      "('with', 35)\n",
      "[[0 0 1 0 1 1 0 0 0 1 0 0 0 0 2 1 1 1 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 1 1 0 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 0 1 1]\n",
      " [1 0 2 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 2 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "count = CountVectorizer()\n",
    "docs = np.array(['I really like Kris Kristofferson and his usual easy going delivery of lines in his movies',\n",
    "                 'Age has helped him with his soft spoken low energy style and he will steal a scene effortlessly',\n",
    "                 'I must give kudos to the cinematography and and the actors'])  #取自df\n",
    "bag = count.fit_transform(docs)\n",
    "for text in list(sorted(count.vocabulary_.items())):    #索引排列\n",
    "    print(text)\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ccb15",
   "metadata": {},
   "source": [
    "# TF-IDF(詞頻-反向文件頻率)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b9b25d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      " [0.   0.   0.15 0.   0.26 0.26 0.   0.   0.   0.26 0.   0.   0.   0.\n",
      " 0.4  0.26 0.26 0.26 0.   0.26 0.26 0.   0.26 0.   0.26 0.26 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.26 0.   0.  ]\n",
      "2 \n",
      " [0.   0.25 0.15 0.   0.   0.   0.25 0.25 0.   0.   0.25 0.25 0.25 0.25\n",
      " 0.19 0.   0.   0.   0.   0.   0.   0.25 0.   0.   0.   0.   0.25 0.25\n",
      " 0.25 0.25 0.25 0.   0.   0.   0.25 0.25]\n",
      "3 \n",
      " [0.3  0.   0.35 0.3  0.   0.   0.   0.   0.3  0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.3  0.   0.   0.   0.   0.3  0.   0.   0.   0.\n",
      " 0.   0.   0.   0.59 0.3  0.   0.   0.  ]\n"
     ]
    }
   ],
   "source": [
    "#確認關聯性 可供資料分離 ('and', 2) = 0.35 較無關聯 3個資料都有\n",
    "tfidf = TfidfVectorizer(use_idf = True,\n",
    "                        norm = 'l2',\n",
    "                        smooth_idf = True)\n",
    "np.set_printoptions(precision = 2)\n",
    "for idx,texts in enumerate(tfidf.fit_transform(docs).toarray()):\n",
    "    print(idx+1,'\\n',texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97419b9",
   "metadata": {},
   "source": [
    "# 文字清理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dd6307f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nd three more acting performances (including Yam).'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0,'review'][-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffca6fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nd three more acting performances  including yam  '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regular expression(re) 正規表式是\n",
    "def preprocessor(text1):\n",
    "    text1 = re.sub('<[^>]*>', '',text1)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text1)\n",
    "    text1 = (re.sub('[\\W]',' ',text1.lower()) + ' '.join(emoticons).replace('-',''))\n",
    "    return text1\n",
    "preprocessor(df.loc[0,'review'][-50:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b8cd4f",
   "metadata": {},
   "source": [
    "# 字符轉換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce82ba24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nd', 'three', 'more', 'acting', 'performances', 'including', 'yam']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(text1):\n",
    "    return text1.split()\n",
    "print(tokenizer(preprocessor(df.loc[0,'review'][-50:])))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11485cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run', 'processes', 'tow', 'cup', 'of', 'tea']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text1):\n",
    "    return [porter.stem(word) for word in tokenizer(text1)]\n",
    "#tokenizer_porter(preprocessor(df.loc[0,'review'][-50:]))\n",
    "tokenizer_porter('running processesing tow cups of tea')  #還原字根形式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38bbbbc",
   "metadata": {},
   "source": [
    "# 停用字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "8c6dd0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MSI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#下載停用詞 可自己新增\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e668169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love', 'three', 'act', 'perform', 'includ', 'yam']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "[w for w in tokenizer_porter((preprocessor(df.loc[0,'review'][0:])))[-10:] if w not in stop] #取最後10個字 並刪除停用詞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66028a62",
   "metadata": {},
   "source": [
    "# 訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11ad4432",
   "metadata": {},
   "outputs": [],
   "source": [
    "#訓練模型需要模組\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fd109c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df.loc[:25000,'review'].values\n",
    "y_train = df.loc[:25000,'sentiment'].values\n",
    "x_test = df.loc[25000:,'review'].values\n",
    "y_test = df.loc[25000:,'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a8935de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        TfidfVectorizer(lowercase=False)),\n",
       "                                       ('clf',\n",
       "                                        LogisticRegression(random_state=0,\n",
       "                                                           solver='liblinear'))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'clf__C': [1.0, 10.0, 100.0],\n",
       "                          'clf__penalty': ['l1', 'l2'],\n",
       "                          'vect__ngram_range': [(1, 1)],\n",
       "                          'vect__stop_words': [['i', 'me', 'my', 'myself', 'we',\n",
       "                                                'our', 'ours', 'ourselves',\n",
       "                                                'you', \"you're\", \"you've...\n",
       "                                                'yourselves', 'he', 'him',\n",
       "                                                'his', 'himself', 'she',\n",
       "                                                \"she's\", 'her', 'hers',\n",
       "                                                'herself', 'it', \"it's\", 'its',\n",
       "                                                'itself', ...],\n",
       "                                               None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x000002A98406EB80>,\n",
       "                                              <function tokenizer_porter at 0x000002A9A440BDC0>]},\n",
       "                         {'clf__C': [1.0, 10.0, 100.0],\n",
       "                          'clf__penalty': ['l1', 'l2'], 'vect__norm': [None],\n",
       "                          'vect__use_idf': [False]}],\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents = None,\n",
    "                        lowercase = False,\n",
    "                        preprocessor = None)\n",
    "\n",
    "param_grid = [{'vect__ngram_range':[(1,1)],\n",
    "               'vect__stop_words':[stop,None],        #停用字\n",
    "               'vect__tokenizer':[tokenizer,\n",
    "                                  tokenizer_porter],\n",
    "               'clf__penalty':['l1','l2'],\n",
    "               'clf__C':[1.0,10.0,100.0]},\n",
    "              {'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__penalty':['l1','l2'],\n",
    "               'clf__C':[1.0,10.0,100.0]}]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect',tfidf),\n",
    "                     ('clf',LogisticRegression(random_state = 0 ,solver = 'liblinear'))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf , param_grid,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 5, verbose = 1 ,\n",
    "                           n_jobs = -1)\n",
    "gs_lr_tfidf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27ec1883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳參數: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x000001E16B337DC0>}\n",
      "train acc: 0.885\n",
      "test acc: 0.890\n"
     ]
    }
   ],
   "source": [
    "print('最佳參數: %s' % gs_lr_tfidf.best_params_)\n",
    "print('train acc: %.3f' % gs_lr_tfidf.best_score_)\n",
    "\n",
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print('test acc: %.3f' % clf.score(x_test,y_test))   #使用最佳參數預測數據\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce0ccc2",
   "metadata": {},
   "source": [
    "# 核外學習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c625c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "if not os.path.isfile('movie_data.csv'):\n",
    "    if not os.path.isfile('movie_data.csv.gz'):\n",
    "        print('not exist')\n",
    "    else:\n",
    "        with gzip.open('movie_data.csv.gz', 'rb') as in_f,open('movie_data.csv', 'wb') as out_f:\n",
    "            out_f.write(in_f.read())\n",
    "            \n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv)        #skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label\n",
    "            \n",
    "\n",
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc568a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:20\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from distutils.version import LooseVersion as Version\n",
    "from sklearn import __version__ as sklearn_version\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore', \n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None, \n",
    "                         tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "clf = SGDClassifier(loss='log', random_state=1)                   #隨機梯度\n",
    "\n",
    "\n",
    "doc_stream = stream_docs(path='movie_data.csv')\n",
    "\n",
    "import pyprind        #進度條\n",
    "pbar = pyprind.ProgBar(45)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):                                               #45小批文件每小批為1000個文件\n",
    "    x_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not x_train:\n",
    "        break\n",
    "    x_train = vect.transform(x_train)\n",
    "    clf.partial_fit(x_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24932824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.866\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = get_minibatch(doc_stream, size=5000)                  #5000個文件評估效能           \n",
    "x_test = vect.transform(x_test)\n",
    "print('Accuracy: %.3f' % clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab7c10a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.partial_fit(x_test, y_test)  #最後5000個文件更新模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9a69d3",
   "metadata": {},
   "source": [
    "# 主題建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fe0c992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "worst minutes awful script stupid\n",
      "Topic 2:\n",
      "family mother father children girl\n",
      "Topic 3:\n",
      "american war dvd music history\n",
      "Topic 4:\n",
      "human audience cinema art feel\n",
      "Topic 5:\n",
      "police guy car dead murder\n",
      "Topic 6:\n",
      "horror house sex blood gore\n",
      "Topic 7:\n",
      "role performance comedy actor performances\n",
      "Topic 8:\n",
      "series episode episodes tv season\n",
      "Topic 9:\n",
      "book version original effects read\n",
      "Topic 10:\n",
      "action fight guy fun guys\n",
      "\n",
      "Horror movie #1:\n",
      "Emilio Miraglia's first Giallo feature, The Night Evelyn Came Out of the Grave, was a great combination of Giallo and Gothic horror - and this second film is even better! We've got more of the Giallo side of the equation this time around, although Miraglia doesn't lose the Gothic horror stylings tha ...\n",
      "\n",
      "Horror movie #2:\n",
      "This film marked the end of the \"serious\" Universal Monsters era (Abbott and Costello meet up with the monsters later in \"Abbott and Costello Meet Frankentstein\"). It was a somewhat desparate, yet fun attempt to revive the classic monsters of the Wolf Man, Frankenstein's monster, and Dracula one \"la ...\n",
      "\n",
      "Horror movie #3:\n",
      "This film marked the end of the \"serious\" Universal Monsters era (Abbott and Costello meet up with the monsters later in \"Abbott and Costello Meet Frankentstein\"). It was a somewhat desparate, yet fun attempt to revive the classic monsters of the Wolf Man, Frankenstein's monster, and Dracula one \"la ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI\\anaconda3\\envs\\mypython\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "df.head(3)\n",
    "\n",
    "count = CountVectorizer(stop_words='english',\n",
    "                        max_df=.1,\n",
    "                        max_features=5000)\n",
    "X = count.fit_transform(df['review'].values)\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10,\n",
    "                                random_state=123,\n",
    "                                learning_method='batch')\n",
    "X_topics = lda.fit_transform(X)\n",
    "\n",
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx + 1))\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()\\\n",
    "                        [:-n_top_words - 1:-1]]))\n",
    "    \n",
    "horror = X_topics[:, 5].argsort()[::-1]\n",
    "\n",
    "for iter_idx, movie_idx in enumerate(horror[:3]):\n",
    "    print('\\nHorror movie #%d:' % (iter_idx + 1))\n",
    "    print(df['review'][movie_idx][:300], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29035db",
   "metadata": {},
   "source": [
    "# 序列化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0fac94ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "dest = os.path.join('movieclassifier', 'pkl_objects')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "\n",
    "pickle.dump(stop, open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol=4)   \n",
    "pickle.dump(clf, open(os.path.join(dest, 'classifier.pkl'), 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "98bb3f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = pickle.load(open(os.path.join('pkl_objects', 'classifier.pkl'), 'rb')) #反序列化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "32338fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: negative\n",
      "Probability: 77.53%\n"
     ]
    }
   ],
   "source": [
    "#執行情緒分類\n",
    "label = {0:'negative', 1:'positive'}\n",
    "\n",
    "example = [str(preprocessor(df.loc[1,'review']))]\n",
    "x = vect.transform(example)\n",
    "print('Prediction: %s\\nProbability: %.2f%%' %\\\n",
    "      (label[clf.predict(x)[0]], \n",
    "       np.max(clf.predict_proba(x))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2b83fda5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i was just watching a forensic files marathon on court tv  the episode was identical to the plot of this movie  right down to the incest secret and the affair with the sister subplot  i don t recall any based on a true story disclaimer  but the case does have mow written all over it  apparently it chronicles the real homicide of ruby morris by her husband earl  sentenced to 25 years to life for her murder  just goes to show you  truth can be stranger than fiction  because i thought the lifetime plot was contrived and a more than  a stretch  insofar as believability goes  i m with the other posters who said the acting was bad  i didn t notice it with all of the players  though  it was really the lead character  the daughter  whose performance was bad '"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(df.loc[1,'review']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7096e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
